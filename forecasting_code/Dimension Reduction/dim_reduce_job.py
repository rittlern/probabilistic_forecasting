from DimReduceNet import *

# training parameters; supply a single value if you want value fixed throughout training, or a list if dynamic
num_training_phases = 1  # each training phrase can come with it's own parameter settings

training_params = {

    # architecture parameters
    'linear': False,  # learn a linear transformation
    'nlayers': 5,  # number of layers
    'layers': None,  # updated below
    'resnet': True,  # learn a linear skip connection that get summed with the output of the rest of the network

    # core training parameters
    'm': 2,  # final dimensional target for predictors. TRAINING EASIER IF SAME DIM AS RESPONSE
    'gaussian_assumption': True,  # assume the data are generated by a joint Gaussian in the training?
    'num_steps': 50,  # steps in each phase in the training  #usually 200 good
    'batch_size': 500,  # mini-batch size
    'num_batches_to_accum': 15,  # number of gradients to sample/accumulate before updating parameters
    'learning_rate': .02,  # learning rate for each phase in the training
    'synthetic_data': False,  # are the data synthetic? used only for read/save directory, formatting, etc.

    # secondary training parameters
    'unique': False,  # learn a unique transformation?
    'estimate_var_baseline': True,  # estimate the optimal var under gaussian assumption; no need when using synthetic
    'normalize': True,  # normalize batches
    'max_batch': False,  # if True, take all of the training data in each batch, regardless of batch_size setting
    'training_seed': 10,  # seed used for initialization and mini-batching
    'beta_1': .999,  # Adam parameters; use of large beta_1 for knn (~.999) is recommended
    'beta_2': .9999,  # Adam parameters
    'training_phases': list(range(num_training_phases)),  # phases of the training

    # parameters for non-parameteric estimation of densities
    'est_type': 'knn',  # type of nonparametric estimation to use; options are 'knn','kde' and 'kde_gen'
    'k_yz': 80,  # how many nearest neighbors to use in density estimation of (y,z) = (y,Tx)
    'k_z': 100,  # how many nearest neighbors to use in density estimation of z = Tx
    'h': 2,  # smoothing parameters for kde. should be a numpy array of length m+p or a float_dim)
    'A': None,  # cov mat for gaussian kernel used in kde, up to constant multiple. should be (m+p) x (m+p)

    # parameters for naive approaches (used when running naive_dim_reduce.py); uses 'm' as above for defining low dim
    'naive': False,  # is dimension reduction done naively (for calling 'dim_reduce_naive.py')
    'naive_method': 'top_k',  # for now, either 'pca' or 'top_k'; top_k selects most correlated locales, pca uses pca

    # settings for resumption of training/ training on old data
    'restart': None,   #'Real_Experiments/run_123/',  # None, or a string specifying a directory containing chk folder to restart on
    'data_directory': None  #'Real_Experiments/run_123/',  # subdir of Experiments or Real_Experiments where some interesting data live; None default
}


# get the dimension of the predictors we want to map to lower dimensional space
stem = 'Experiments/' if training_params['synthetic_data'] else 'Real_Experiments/'

if training_params['restart'] is not None:
    path = training_params['restart']
    save_path = training_params['restart']
else:
    path, save_path = path_finder(stem, training_params['data_directory'])

with open(path + 'x_train.npy', 'rb') as f:
    major, minor = np.lib.format.read_magic(f)
    shape, _, _ = np.lib.format.read_array_header_1_0(f)

d = shape[1]
m = training_params['m']

# set up network architecture
nlayers = training_params['nlayers']
layers = list()
if training_params['linear']:
    for i in range(nlayers):
        if i != (nlayers - 1):
            layers += [FullyConnected("FC_%3.3d" % i, d, d, initial_mat=None, train_bias=False)]
        else:
            layers += [FullyConnected("FC_%3.3d" % i, d, m, initial_mat=None, train_bias=False)]

else:
    for i in range(nlayers):
        if i != (nlayers - 1):
            layers += [FullyConnected("FC_%3.3d" % i, d, d, initial_mat=None, train_bias=False),
                        PReLU("PReLU_%3.3d" % i, size=d)]
        else:
            layers += [FullyConnected("FC_%3.3d" % i, d, m, initial_mat=None, train_bias=False)]

training_params['layers'] = layers
